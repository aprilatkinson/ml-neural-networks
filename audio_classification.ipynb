{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd9c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kernel started\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Kernel started\")\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# 1) Load small local slice (NOT streaming)\n",
    "ds = load_dataset(\"superb\", \"ks\", split=\"test[:20]\")\n",
    "\n",
    "# 2) Decode audio using librosa/soundfile\n",
    "ds = ds.cast_column(\"audio\", Audio(decode=True))\n",
    "\n",
    "print(\"rows:\", len(ds))\n",
    "print(\"keys:\", ds.column_names)\n",
    "\n",
    "# 3) Load pre-trained audio classifier\n",
    "clf = pipeline(\"audio-classification\", model=\"superb/hubert-base-superb-ks\")\n",
    "\n",
    "# 4) Predict on 1 sample (demo)\n",
    "sample = ds[0]\n",
    "audio = sample[\"audio\"]\n",
    "true_label = ds.features[\"label\"].names[sample[\"label\"]]\n",
    "\n",
    "preds = clf({\"array\": audio[\"array\"], \"sampling_rate\": audio[\"sampling_rate\"]})\n",
    "top = preds[0]\n",
    "\n",
    "print(\"\\n--- Single sample ---\")\n",
    "print(\"True:\", true_label)\n",
    "print(\"Pred:\", top[\"label\"], \"conf:\", round(top[\"score\"], 3))\n",
    "\n",
    "# 5) Evaluate accuracy on 20 samples\n",
    "correct = 0\n",
    "for i in range(len(ds)):\n",
    "    s = ds[i]\n",
    "    a = s[\"audio\"]\n",
    "    true = ds.features[\"label\"].names[s[\"label\"]]\n",
    "    pred = clf({\"array\": a[\"array\"], \"sampling_rate\": a[\"sampling_rate\"]})[0][\"label\"]\n",
    "    if pred == true:\n",
    "        correct += 1\n",
    "\n",
    "N = 20  # evaluate on 20 samples (fast + works with streaming)\n",
    "acc = correct / N\n",
    "print(f\"✅ Accuracy on {N} samples: {acc:.2%} ({correct}/{N})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0057db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok sample 1 label_id: 10\n",
      "ok sample 2 label_id: 10\n",
      "ok sample 3 label_id: 10\n",
      "✅ done\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"superb\", \"ks\", split=\"test\", streaming=True)\n",
    "\n",
    "n = 0\n",
    "for sample in ds.take(3):\n",
    "    n += 1\n",
    "    print(\"ok sample\", n, \"label_id:\", sample[\"label\"])\n",
    "print(\"✅ done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfda7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: _silence_\n",
      "Sampling rate: 16000\n",
      "Num samples: 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/hubert-base-superb-ks were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at superb/hubert-base-superb-ks and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "# 1) Load ONE sample (streaming-safe)\n",
    "ds = load_dataset(\"superb\", \"ks\", split=\"test\", streaming=True)\n",
    "\n",
    "sample = next(iter(ds))\n",
    "audio = sample[\"audio\"]\n",
    "label_id = sample[\"label\"]\n",
    "true_label = ds.features[\"label\"].names[label_id]\n",
    "\n",
    "print(\"True label:\", true_label)\n",
    "print(\"Sampling rate:\", audio[\"sampling_rate\"])\n",
    "print(\"Num samples:\", len(audio[\"array\"]))\n",
    "\n",
    "# 2) Load the pretrained model (keyword spotting)\n",
    "model_id = \"superb/hubert-base-superb-ks\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "# 3) Prepare input + predict\n",
    "x = np.asarray(audio[\"array\"], dtype=np.float32)\n",
    "sr = int(audio[\"sampling_rate\"])\n",
    "\n",
    "inputs = feature_extractor(x, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0]\n",
    "\n",
    "top_idx = int(torch.argmax(probs))\n",
    "pred_label = model.config.id2label[top_idx]\n",
    "pred_conf = float(probs[top_idx])\n",
    "\n",
    "print(\"\\nPrediction:\", pred_label, \"| confidence:\", round(pred_conf, 3))\n",
    "print(\"Match:\", pred_label == true_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e2938c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "still alive ✅\n"
     ]
    }
   ],
   "source": [
    "print(\"still alive ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21b4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/hubert-base-superb-ks were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at superb/hubert-base-superb-ks and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "ds = load_dataset(\"superb\", \"ks\", split=\"test\", streaming=True)\n",
    "sample = next(iter(ds))\n",
    "\n",
    "audio = sample[\"audio\"]\n",
    "true_label = ds.features[\"label\"].names[sample[\"label\"]]\n",
    "\n",
    "model_id = \"superb/hubert-base-superb-ks\"\n",
    "fe = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id).to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "x = np.asarray(audio[\"array\"], dtype=np.float32)\n",
    "sr = int(audio[\"sampling_rate\"])\n",
    "\n",
    "inputs = fe(x, sampling_rate=sr, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    probs = torch.softmax(model(**inputs).logits, dim=-1)[0]\n",
    "\n",
    "top5 = torch.topk(probs, k=5)\n",
    "for score, idx in zip(top5.values, top5.indices):\n",
    "    label = model.config.id2label[int(idx)]\n",
    "    print(f\"{label:15s} {float(score):.3f}\")\n",
    "\n",
    "pred = model.config.id2label[int(torch.argmax(probs))]\n",
    "print(\"\\nTrue:\", true_label)\n",
    "print(\"Pred:\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a75e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/hubert-base-superb-ks were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at superb/hubert-base-superb-ks and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification\n",
    "\n",
    "# Load dataset (non-streaming is easiest for small N)\n",
    "ds = load_dataset(\"superb\", \"ks\", split=\"test[:50]\")  # only first 50 to keep it fast\n",
    "\n",
    "model_id = \"superb/hubert-base-superb-ks\"\n",
    "fe = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "N = 10\n",
    "correct = 0\n",
    "\n",
    "for i in range(N):\n",
    "    sample = ds[i]\n",
    "    audio = sample[\"audio\"]\n",
    "    x = np.asarray(audio[\"array\"], dtype=np.float32)\n",
    "    sr = int(audio[\"sampling_rate\"])\n",
    "\n",
    "    inputs = fe(x, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        pred_id = int(torch.argmax(logits, dim=-1))\n",
    "\n",
    "    pred_label = model.config.id2label[pred_id]\n",
    "    true_label = ds.features[\"label\"].names[sample[\"label\"]]\n",
    "\n",
    "    is_correct = (pred_label == true_label)\n",
    "    correct += int(is_correct)\n",
    "\n",
    "    print(f\"{i+1:02d} | true={true_label:15s} pred={pred_label:15s} {'✓' if is_correct else '✗'}\")\n",
    "\n",
    "acc = correct / N\n",
    "print(f\"\\n✅ Accuracy on {N} samples: {acc:.2%} ({correct}/{N})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b9b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ END OF CELL REACHED\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ END OF CELL REACHED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debda74a",
   "metadata": {},
   "source": [
    "## Reflection – Lab M1.04\n",
    "Build Simple Neural Network & Audio Classification\n",
    "\n",
    "Teaching Models to See (Images)\n",
    "How did your neural network learn to distinguish dogs from cats?\n",
    "\n",
    "The neural network learned to distinguish dogs from cats by adjusting its internal weights during training using backpropagation. Each image was flattened into numerical pixel values, and during training the model compared its predictions with the true labels. The error (loss) was used to update weights so that patterns associated with dogs or cats became more strongly represented in the network.\n",
    "\n",
    "Over multiple epochs, the model gradually reduced its loss and improved accuracy by reinforcing features that helped separate the two classes.\n",
    "\n",
    "What patterns do you think it learned?\n",
    "\n",
    "The model likely learned low-level visual patterns such as:\n",
    "\n",
    "edges and contours\n",
    "\n",
    "color distributions\n",
    "\n",
    "texture differences\n",
    "\n",
    "shape density and contrast\n",
    "\n",
    "Because the images were flattened, the model could not explicitly understand spatial structure, but it could still detect recurring pixel combinations that statistically correlate with cats or dogs.\n",
    "\n",
    "Why did flattening the image work? What information might be lost?\n",
    "\n",
    "Flattening worked because it converts images into a format compatible with a simple fully connected neural network. The model can still learn from pixel values, but it loses spatial relationships, such as:\n",
    "\n",
    "relative position of eyes, ears, or body parts\n",
    "\n",
    "local neighborhoods of pixels\n",
    "\n",
    "This is why convolutional neural networks (CNNs) generally perform better on image data.\n",
    "\n",
    "Teaching Models to Understand (Audio)\n",
    "How is audio different from images as input?\n",
    "\n",
    "Audio is a time-based signal, not a spatial one. Instead of pixels arranged in two dimensions, audio consists of waveforms sampled over time. This means:\n",
    "\n",
    "order and timing matter\n",
    "\n",
    "patterns unfold sequentially\n",
    "\n",
    "context depends on surrounding samples\n",
    "\n",
    "Because of this, models must understand temporal dependencies rather than spatial layouts.\n",
    "\n",
    "What do you think the pre-trained audio model learned during training?\n",
    "\n",
    "The pre-trained HuBERT model learned:\n",
    "\n",
    "phonetic and acoustic patterns\n",
    "\n",
    "temporal structure of speech\n",
    "\n",
    "distinctions between silence, words, and sound events\n",
    "\n",
    "During large-scale pretraining, the model learned general audio representations that can later be reused for downstream tasks such as keyword spotting or intent recognition.\n",
    "\n",
    "Why do we use pre-trained models instead of training from scratch?\n",
    "\n",
    "Training audio models from scratch requires:\n",
    "\n",
    "massive datasets\n",
    "\n",
    "high computational cost\n",
    "\n",
    "long training time\n",
    "\n",
    "Pre-trained models allow us to:\n",
    "\n",
    "reuse learned representations\n",
    "\n",
    "achieve good performance with minimal data\n",
    "\n",
    "focus on inference and application rather than raw training\n",
    "\n",
    "This is more efficient and practical for real-world use cases.\n",
    "\n",
    "Transfer Learning\n",
    "Why does the audio model work on a dataset it was not trained on?\n",
    "\n",
    "The model learned general audio features rather than memorizing specific samples. These features (such as frequency patterns and temporal dynamics) transfer well across datasets, allowing the model to perform reasonably even on new tasks.\n",
    "\n",
    "What knowledge was transferred?\n",
    "\n",
    "Transferred knowledge includes:\n",
    "\n",
    "how speech sounds are structured\n",
    "\n",
    "how silence differs from speech\n",
    "\n",
    "how temporal patterns map to semantic meaning\n",
    "\n",
    "This is similar to how humans learn language sounds once and reuse that knowledge in new contexts.\n",
    "\n",
    "How is this similar to human learning?\n",
    "\n",
    "Humans do not relearn how to hear or see from scratch for every task. Similarly, pre-trained models reuse foundational knowledge and adapt it to new problems, which makes learning faster and more robust.\n",
    "\n",
    "Model Architecture\n",
    "Why are the image and audio models structured differently?\n",
    "\n",
    "Image models focus on spatial relationships, while audio models focus on temporal relationships. This difference requires different architectures:\n",
    "\n",
    "dense layers for simple image baselines\n",
    "\n",
    "transformers for sequence modeling in audio\n",
    "\n",
    "What makes transformers good for audio and text?\n",
    "\n",
    "Transformers:\n",
    "\n",
    "handle long-range dependencies\n",
    "\n",
    "process sequences efficiently\n",
    "\n",
    "use attention to focus on relevant parts of the input\n",
    "\n",
    "This makes them especially effective for speech and language tasks.\n",
    "\n",
    "How do convolutional layers differ from transformer layers?\n",
    "\n",
    "Convolutional layers focus on local patterns\n",
    "\n",
    "Transformer layers focus on global context\n",
    "\n",
    "Both are powerful, but transformers are more flexible for sequence-based data like audio and text."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
